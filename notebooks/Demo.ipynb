{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bba8baf0",
   "metadata": {},
   "source": [
    "This notebook is mainly used to demonstrate out project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4414aee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mohammed\\Anaconda3\\envs\\robo2\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW, Adafactor\n",
    "from transformers import T5ForConditionalGeneration,T5Tokenizer\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddcc247",
   "metadata": {},
   "source": [
    "# Question generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79655e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mohammed\\Anaconda3\\envs\\robo2\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:169: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "# Lead the model\n",
    "model = T5ForConditionalGeneration.from_pretrained('ramsrigouthamg/t5_boolean_questions')\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        \n",
    "# generator type       \n",
    "def topkp_decoding (inp_ids,attn_mask):\n",
    "    topkp_output = model.generate(input_ids=inp_ids,\n",
    "            attention_mask=attn_mask,\n",
    "            max_length=256,\n",
    "            do_sample=True,\n",
    "            top_k=40,\n",
    "            top_p=0.80,\n",
    "            num_return_sequences=1,\n",
    "            no_repeat_ngram_size=2,\n",
    "            early_stopping=True\n",
    "           )\n",
    "    Questions = [tokenizer.decode(out, skip_special_tokens=True,clean_up_tokenization_spaces=True) for out in topkp_output]\n",
    "    return [Question.strip().capitalize() for Question in Questions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4451b96d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recipe name: Arugula and Persimmon Salad with Pistachio Vinaigrette\n",
      "\n",
      "In a blender, puree all the vinaigrette ingredients until the mixture is smooth and well incorporated. Toss the arugula in a large bowl with the vinaigrette until the leaves are well coated. Arrange the persimmon slices on two serving plates, then top them with the dressed greens and serve.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mohammed\\Anaconda3\\envs\\robo2\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:215: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n",
      "  f\"This sequence already has {self.eos_token}. In future versions this behavior may lead to duplicated eos tokens being added.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question ['Is arugula and persimmon the same thing?'] answer:  yes\n",
      "Question ['Is persimmon vinaigrette the same as dressing?'] answer:  no\n"
     ]
    }
   ],
   "source": [
    "recipe = r\"\"\"\n",
    "In a blender, puree all the vinaigrette ingredients until the mixture is smooth and well incorporated. Toss the arugula in a large bowl with the vinaigrette until the leaves are well coated. Arrange the persimmon slices on two serving plates, then top them with the dressed greens and serve.\"\"\"\n",
    "\n",
    "print(\"Recipe name: Arugula and Persimmon Salad with Pistachio Vinaigrette\")\n",
    "print(recipe)\n",
    "\n",
    "set_seed(100)\n",
    "Qs = []\n",
    "# generate two qs for each answer\n",
    "for truefalse in ['yes', 'no']:\n",
    "    text = \"truefalse: %s passage: %s </s>\" % (truefalse, recipe)\n",
    "    encoding = tokenizer.encode_plus(text, return_tensors=\"pt\")\n",
    "    input_ids, attention_masks = encoding[\"input_ids\"].to(device), encoding[\"attention_mask\"].to(device)\n",
    "\n",
    "    output = topkp_decoding(input_ids,attention_masks)\n",
    "    Qs.append(output)\n",
    "    print (\"Question\", output, \"answer: \",truefalse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b3e46a",
   "metadata": {},
   "source": [
    "# Question generation tricky case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55273a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recipe name: Big-Batch Parmesan Polenta\n",
      "\n",
      "Bring 9 cups water to a boil in a medium pot over high. Add salt and reduce heat to medium-low. Stirring constantly with a wooden spoon, gradually stream in polenta. Cook, stirring often, until thick and creamy, 30â€“35 minutes. Remove from heat and stir in Parmesan and pepper.salt and red pepper flakes and scatter herbs over.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "recipe2 = r\"\"\"\n",
    "Bring 9 cups water to a boil in a medium pot over high. Add salt and reduce heat to medium-low. Stirring constantly with a wooden spoon, gradually stream in polenta. Cook, stirring often, until thick and creamy, 30â€“35 minutes. Remove from heat and stir in Parmesan and pepper.salt and red pepper flakes and scatter herbs over.\n",
    "\"\"\"\n",
    "print(\"Recipe name: Big-Batch Parmesan Polenta\")\n",
    "print(recipe2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ea4a2f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question ['Does polenta have to be simmered to thicken?'] answer:  yes\n",
      "Question ['Does the polenta have to be melted?'] answer:  no\n",
      "\n",
      "Question ['Does polenta have to be cooked to thaw?'] answer:  yes\n",
      "Question ['Is polenta the same as pfizer polar?'] answer:  no\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for seeds in [41, 43]:\n",
    "    set_seed(seeds)\n",
    "    \n",
    "    for truefalse in ['yes', 'no']:\n",
    "        text = \"truefalse: %s passage: %s </s>\" % (truefalse, recipe2)\n",
    "        encoding = tokenizer.encode_plus(text, return_tensors=\"pt\")\n",
    "        input_ids, attention_masks = encoding[\"input_ids\"].to(device), encoding[\"attention_mask\"].to(device)\n",
    "\n",
    "        output = topkp_decoding(input_ids,attention_masks)\n",
    "        print (\"Question\", output, \"answer: \",truefalse)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d407f067",
   "metadata": {},
   "source": [
    "# Question answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53770788",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(tokenizer, questions, passages, max_length):\n",
    "    \"\"\"Encode the question/passage pairs into features than can be fed to the model.\"\"\"\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for question, passage in zip(questions, passages):\n",
    "        encoded_data = tokenizer.encode_plus(question, passage, max_length=max_length, pad_to_max_length=True, truncation_strategy=\"longest_first\")\n",
    "        encoded_pair = encoded_data[\"input_ids\"]\n",
    "        attention_mask = encoded_data[\"attention_mask\"]\n",
    "\n",
    "        input_ids.append(encoded_pair)\n",
    "        attention_masks.append(attention_mask)\n",
    "\n",
    "    return np.array(input_ids), np.array(attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a41cde4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In a blender, puree all the vinaigrette ingredients until the mixture is smooth and well incorporated. Toss the arugula in a large bowl with the vinaigrette until the leaves are well coated. Arrange the persimmon slices on two serving plates, then top them with the dressed greens and serve. \n",
      "\n",
      "Question: Is arugula and persimmon the same thing?, Yes: 0.4, No: 0.6\n",
      "Question: Is persimmon vinaigrette the same as dressing?, Yes: 0.31, No: 0.69\n"
     ]
    }
   ],
   "source": [
    "# model.load_state_dict(torch.load('model/2ksamplesAcc55.h5'))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "answering_model = AutoModelForSequenceClassification.from_pretrained(\"shahrukhx01/roberta-base-boolq\")\n",
    "answering_model.load_state_dict(torch.load('../models/robertaBool6175.h5', map_location=device))\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"shahrukhx01/roberta-base-boolq\")\n",
    "answering_model.to(device)\n",
    "\n",
    "def predict(question, passage):\n",
    "    sequence = tokenizer.encode_plus(question, passage, return_tensors=\"pt\")['input_ids'].to(device)\n",
    "  \n",
    "    logits = answering_model(sequence)[0]\n",
    "    probabilities = torch.softmax(logits, dim=1).detach().cpu().tolist()[0]\n",
    "    proba_yes = round(probabilities[1], 2)\n",
    "    proba_no = round(probabilities[0], 2)\n",
    "\n",
    "    print(f\"Question: {question}, Yes: {proba_yes}, No: {proba_no}\")\n",
    "\n",
    "print(recipe, \"\\n\")\n",
    "for i in Qs:\n",
    "    predict(i[0], recipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67303775",
   "metadata": {},
   "source": [
    "<h3>Your questions about the recipe </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6cf4d534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your questions? do i need a blender\n",
      "Question: do i need a blender, Yes: 0.41, No: 0.59\n",
      "\n",
      "Enter your questions? Do you need a blender\n",
      "Question: Do you need a blender, Yes: 0.47, No: 0.53\n",
      "\n",
      "Enter your questions? blender\n",
      "Question: blender, Yes: 0.52, No: 0.48\n",
      "\n",
      "Enter your questions? is Arrange the persimmon slices on two serving plates\n",
      "Question: is Arrange the persimmon slices on two serving plates, Yes: 0.53, No: 0.47\n",
      "\n",
      "Enter your questions? are the leaves well coated\n",
      "Question: are the leaves well coated, Yes: 0.43, No: 0.57\n",
      "\n",
      "Enter your questions? well coated\n",
      "Question: well coated, Yes: 0.45, No: 0.55\n",
      "\n",
      "Enter your questions? coated\n",
      "Question: coated, Yes: 0.47, No: 0.53\n",
      "\n",
      "Enter your questions? well\n",
      "Question: well, Yes: 0.43, No: 0.57\n",
      "\n",
      "Enter your questions? quit\n",
      "Good bye!!\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    q = input(\"Enter your questions? \")\n",
    "    if(q == \"quit\"):\n",
    "        print(\"Good bye!!\")\n",
    "        break\n",
    "    predict(q, recipe)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952e2124",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
