{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b439975e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mohammed\\Anaconda3\\envs\\robo2\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddefc7a",
   "metadata": {},
   "source": [
    "### In the cell below, the model can be changed, as well as the learning rate and optimiser.\n",
    "\n",
    "Any model from https://huggingface.co/models?sort=downloads can be passed to the <i>AutoTokenizer.from_pretrained</i> and <i>AutoModelForSequenceClassification.from_pretrained</i> functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9325e2f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mohammed\\Anaconda3\\envs\\robo2\\lib\\site-packages\\transformers\\optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "# Use CPU or GPU if its detected and Pytorch was installed with Cuda-capabilities\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "random.seed(26)\n",
    "np.random.seed(26)\n",
    "torch.manual_seed(26)\n",
    "\n",
    "# Here, models from  can be downloaded to be retrained\n",
    "# On a specialised dataset. Roberta models work with no modifications, Bert based models need minor modifications\n",
    "# And other models will need more modifications to be retrained using this notebook.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"shahrukhx01/roberta-base-boolq\") \n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"shahrukhx01/roberta-base-boolq\")\n",
    "model.to(device) # Send the model to the GPU if we have one\n",
    "\n",
    "learning_rate = 1e-7\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, eps=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c93d4281",
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_split(df, frac=0.2):\n",
    "    \"\"\"\n",
    "        Splits dataset into test and train sets.\n",
    "        df: Pandas dataframe containing the data\n",
    "        frac (optional): the percentage of data that is taken for the test set, defaults to 0.2 if not set.\n",
    "        returns: tuple of (train_set, test_set)\n",
    "    \"\"\"\n",
    "    \n",
    "    n = int(len(df)*frac)\n",
    "    \n",
    "    # get random sample \n",
    "    test = df.sample(n=n, axis=0)\n",
    "    \n",
    "    # get everything but the test sample\n",
    "    train = df.drop(index=test.index)\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecb3622c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\Users\\Mohammed\\Anaconda3\\envs\\robo2\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "def encode_data(tokenizer, questions, passages, max_length):\n",
    "    \"\"\"\n",
    "        Encode the question/passage pairs into features than can be fed to the model.\n",
    "        tokenizer: AutoTokenizer class from the Transformers package\n",
    "        questions: Questions to be vectorised\n",
    "        passages: Text with context that answers the questions\n",
    "        max_length: maximum length of text sequences, including passages and questions.\n",
    "        returns: Input_ids and attention masks to be passed to the model.\n",
    "    \"\"\"\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    \n",
    "    for question, passage in zip(questions, passages):\n",
    "        encoded_data = tokenizer.encode_plus(question, passage, max_length=max_length, pad_to_max_length=True, truncation_strategy=\"longest_first\")\n",
    "        encoded_pair = encoded_data[\"input_ids\"]\n",
    "        attention_mask = encoded_data[\"attention_mask\"]\n",
    "\n",
    "        input_ids.append(encoded_pair)\n",
    "        attention_masks.append(attention_mask)\n",
    "\n",
    "    return np.array(input_ids), np.array(attention_masks)\n",
    "\n",
    "# Loading data\n",
    "train_df = pd.read_csv('../data/generatedQuestions4k.csv', header=0)\n",
    "train_data_df, dev_data_df = t_split(train_df, frac=0.2)\n",
    "# print(test_df)\n",
    "\n",
    "passages_train = train_data_df.Recipe.values\n",
    "questions_train = train_data_df.Question.values\n",
    "answers_train = train_data_df.label.values.astype(int)\n",
    "\n",
    "passages_dev = dev_data_df.Recipe.values\n",
    "questions_dev = dev_data_df.Question.values\n",
    "answers_dev = dev_data_df.label.values.astype(int)\n",
    "\n",
    "# Encoding data\n",
    "max_seq_length = 256\n",
    "input_ids_train, attention_masks_train = encode_data(tokenizer, questions_train, passages_train, max_seq_length)\n",
    "input_ids_dev, attention_masks_dev = encode_data(tokenizer, questions_dev, passages_dev, max_seq_length)\n",
    "\n",
    "train_features = (input_ids_train, attention_masks_train, answers_train)\n",
    "dev_features = (input_ids_dev, attention_masks_dev, answers_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5401c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building Dataloaders\n",
    "batch_size = 8\n",
    "\n",
    "train_features_tensors = [torch.tensor(feature, dtype=torch.long) for feature in train_features]\n",
    "dev_features_tensors = [torch.tensor(feature, dtype=torch.long) for feature in dev_features]\n",
    "\n",
    "train_dataset = TensorDataset(*train_features_tensors)\n",
    "dev_dataset = TensorDataset(*dev_features_tensors)\n",
    "\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "dev_sampler = SequentialSampler(dev_dataset)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=batch_size)\n",
    "dev_dataloader = DataLoader(dev_dataset, sampler=dev_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6b9966",
   "metadata": {},
   "source": [
    "<h5> In the cell above, the dataloader settings can be changed, including batch size and type of sampler (Random, Sequential), and in the cell below the number epochs can be changed and the training variables can be accessed </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc31609e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   0%|                                                                                     | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 1 0 1 0 0]\n",
      "[1 1 1 0 1 1 0 0]\n",
      "[1 0 0 0 1 0 1 1]\n",
      "[1 0 0 0 0 0 0 0]\n",
      "[0 0 1 1 1 0 0 0]\n",
      "[0 0 1 1 1 0 0 1]\n",
      "[1 0 1 0 1 1 0 1]\n",
      "[1 0 0 1 0 0 0 0]\n",
      "[0 0 0 0 1 0 0 0]\n",
      "[0 0 0 1 1 0 1 1]\n",
      "[0 0 1 0 1 0 0 0]\n",
      "[0 1 1 1 1 0 0 0]\n",
      "[1 1 0 1 1 1 1 1]\n",
      "[0 0 1 1 0 1 1 0]\n",
      "[1 0 0 0 1 1 0 0]\n",
      "[0 0 1 1 1 1 1 0]\n",
      "[0 0 1 0 0 0 1 0]\n",
      "[0 0 0 1 1 0 0 1]\n",
      "[1 1 0 0 1 1 0 1]\n",
      "[0 0 1 1 1 1 1 1]\n",
      "[1 1 0 1 1 0 0 0]\n",
      "[0 1 0 0 1 0 1 0]\n",
      "[1 0 0 0 0 0 1 1]\n",
      "[1 0 1 1 0 1 0 1]\n",
      "[0 0 0 0 1 0 0 0]\n",
      "[0 0 1 0 0 0 1 1]\n",
      "[0 1 1 1 1 0 1 0]\n",
      "[1 1 1 1 1 0 0 1]\n",
      "[1 0 1 0 0 0 0 0]\n",
      "[0 0 0 1 1 1 1 1]\n",
      "[0 1 0 1 1 0 1 1]\n",
      "[0 0 0 1 0 0 1 0]\n",
      "[0 1 0 1 1 1 1 1]\n",
      "[0 0 0 0 0 1 0 1]\n",
      "[0 1 0 1 0 0 1 1]\n",
      "[0 0 1 1 1 0 1 1]\n",
      "[0 1 0 1 1 0 0 0]\n",
      "[0 0 0 1 0 1 1 1]\n",
      "[1 1 0 1 1 0 0 1]\n",
      "[0 1 1 0 0 1 0 0]\n",
      "[0 0 0 0 0 0 0 1]\n",
      "[0 0 0 1 1 1 0 1]\n",
      "[0 1 1 0 1 0 1 1]\n",
      "[0 0 1 1 1 0 0 0]\n",
      "[1 1 1 1 0 1 1 0]\n",
      "[1 0 0 1 1 0 1 0]\n",
      "[0 0 0 1 0 0 0 0]\n",
      "[1 1 1 1 1 0 0 1]\n",
      "[0 1 0 1 1 0 0 1]\n",
      "[0 1 1 1 0 1 1 1]\n",
      "[1 0 1 0 0 0 0 1]\n",
      "[1 0 0 0 0 0 1 0]\n",
      "[0 1 0 0 1 0 1 1]\n",
      "[0 0 1 0 1 1 0 0]\n",
      "[0 0 0 0 0 0 0 0]\n",
      "[0 1 0 0 0 0 1 1]\n",
      "[1 1 0 1 0 1 0 1]\n",
      "[1 1 0 0 0 1 0 1]\n",
      "[0 1 0 0 1 0 0 1]\n",
      "[1 1 1 0 1 1 1 1]\n",
      "[1 1 0 0 0 1 1 0]\n",
      "[0 1 0 1 1 0 0 1]\n",
      "[1 1 0 0 0 0 0 1]\n",
      "[0 0 0 1 0 0 0 1]\n",
      "[1 1 0 1 0 0 1 0]\n",
      "[1 1 1 0 1 1 1 1]\n",
      "[1 0 1 0 1 0 0 0]\n",
      "[0 1 1 0 1 1 1 1]\n",
      "[1 0 0 0 0 0 1 0]\n",
      "[0 0 0 1 1 0 0 1]\n",
      "[0 0 1 0 0 1 0 0]\n",
      "[1 1 0 1 0 0 0 0]\n",
      "[1 1 0 1 1 0 0 0]\n",
      "[1 1 1 0 1 1 0 0]\n",
      "[1 1 0 0 1 0 0 0]\n",
      "[1 0 1 0 1 1 0 1]\n",
      "[1 1 0 0 0 0 0 1]\n",
      "[1 0 1 0 0 0 1 1]\n",
      "[1 1 0 0 0 0 1 0]\n",
      "[0 1 0 0 0 0 1 0]\n",
      "[0 1 1 0 0 0 0 0]\n",
      "[0 0 0 1 0 0 0 0]\n",
      "[0 0 0 0 0 0 1 1]\n",
      "[0 0 0 1 1 0 0 0]\n",
      "[0 1 0 0 1 1 1 1]\n",
      "[1 1 0 1 1 0 1 0]\n",
      "[0 0 1 1 0 1 1 0]\n",
      "[0 0 1 0 0 1 0 1]\n",
      "[0 0 1 0 1 0 1 1]\n",
      "[1 0 1 0 0 0 1 0]\n",
      "[1 0 1 0 1 1 1 0]\n",
      "[0 1 0 0 1 0 0 1]\n",
      "[1 1 0 1 0 0 0 0]\n",
      "[0 0 1 0 0 1 0 0]\n",
      "[1 1 0 1 0 1 0 0]\n",
      "[0 0 0 1 0 0 1 1]\n",
      "[1 0 1 0 1 1 0 0]\n",
      "[0 0 1 0 0 0 1 0]\n",
      "[0 0 1 0 1 1 0 0]\n",
      "[1 1 0 1 0 0 0 0]\n",
      "[0 0 1 1 0 1 1 0]\n",
      "[1 0 1 0 0 0 0 0]\n",
      "[0 1 0 0 0 1 1 1]\n",
      "[0 0 0 0 1 1 1 1]\n",
      "[0 1 1 0 1 1 1 1]\n",
      "[1 0 0 0 1 0 0 1]\n",
      "[1 0 1 0 1 1 0 1]\n",
      "[1 0 0 0 1 1 1 1]\n",
      "[0 1 0 1 0 1 1 1]\n",
      "[0 1 0 1 1 1 0 0]\n",
      "[1 1 0 0 0 0 1 1]\n",
      "[0 1 0 0 0 0 1 0]\n",
      "[1 0 1 1 0 1 0 1]\n",
      "[1 1 1 1 1 1 0 0]\n",
      "[0 1 1 1 1 1 1 0]\n",
      "[0 1 1 1 0 0 1 0]\n",
      "[1 0 0 1 0 1 1 0]\n",
      "[1 1 1 1 0 1 1 0]\n",
      "[0 0 0 0 1 1 1 0]\n",
      "[0 0 0 0 1 1 1 0]\n",
      "[0 1 0 1 0 0 1 1]\n",
      "[0 1 1 1 1 1 1 1]\n",
      "[0 1 1 0 0 1 1 1]\n",
      "[0 0 0 0 0 1 0 1]\n",
      "[1 1 0 1 1 1 0 0]\n",
      "[0 0 0 1 0 1 0 0]\n",
      "[1 0 0 1 1 1 1 0]\n",
      "[0 0 0 0 1 1 1 0]\n",
      "[1 1 0 1 0 0 0 1]\n",
      "[0 1 1 0 1 1 0 1]\n",
      "[0 1 1 0 0 1 1 0]\n",
      "[1 1 1 0 0 1 1 1]\n",
      "[0 0 0 0 0 0 1 0]\n",
      "[1 0 0 1 0 1 1 1]\n",
      "[1 0 0 0 1 1 1 0]\n",
      "[0 1 1 1 1 1 0 1]\n",
      "[0 0 1 1 0 1 0 0]\n",
      "[0 0 1 0 0 0 0 1]\n",
      "[1 1 0 0 1 0 1 0]\n",
      "[0 0 1 0 1 1 0 0]\n",
      "[1 1 1 0 1 0 1 0]\n",
      "[0 1 1 0 1 1 0 0]\n",
      "[0 0 0 1 1 1 0 1]\n",
      "[1 1 1 1 1 1 0 0]\n",
      "[0 0 0 0 0 0 1 1]\n",
      "[1 0 0 0 1 1 1 1]\n",
      "[1 0 0 0 0 1 0 0]\n",
      "[1 1 0 0 1 0 0 0]\n",
      "[1 1 0 0 0 1 0 1]\n",
      "[1 0 1 0 1 0 1 0]\n",
      "[0 1 0 1 0 0 1 0]\n",
      "[0 0 0 0 1 1 1 0]\n",
      "[0 1 1 0 0 0 0 1]\n",
      "[1 1 1 0 0 0 0 0]\n",
      "[1 0 0 1 0 1 0 0]\n",
      "[1 0 0 1 1 1 0 0]\n",
      "[0 1 0 0 1 1 1 0]\n",
      "[1 0 1 0 0 1 1 1]\n",
      "[0 1 1 1 1 0 0 0]\n",
      "[1 1 1 1 0 1 1 1]\n",
      "[1 0 0 1 1 1 0 0]\n",
      "[0 1 0 0 0 1 1 1]\n",
      "[1 1 1 1 1 1 0 1]\n",
      "[0 1 1 0 0 1 0 1]\n",
      "[1 1 1 0 1 1 0 1]\n",
      "[1 1 1 0 1 0 0 1]\n",
      "[1 1 1 0 0 1 0 1]\n",
      "[1 1 1 1 0 0 1 0]\n",
      "[0 0 0 1 1 0 0 1]\n",
      "[1 1 0 1 1 1 0 1]\n",
      "[1 0 1 0 0 1 1 0]\n",
      "[0 1 1 0 1 0 1 1]\n",
      "[1 0 0 1 0 1 1 0]\n",
      "[1 1 0 1 1 1 0 0]\n",
      "[0 1 1 1 1 0 0 1]\n",
      "[1 1 0 0 1 0 1 0]\n",
      "[0 1 1 0 0 0 0 0]\n",
      "[0 0 1 0 1 0 0 1]\n",
      "[1 1 1 0 1 1 0 1]\n",
      "[1 0 0 0 0 0 0 1]\n",
      "[1 1 0 1 1 0 1 1]\n",
      "[1 0 0 0 0 0 0 1]\n",
      "[1 1 1 1 1 0 1 1]\n",
      "[1 1 0 1 1 0 1 1]\n",
      "[0 1 1 1 0 1 1 0]\n",
      "[1 1 1 1 0 1 1 0]\n",
      "[0 0 0 0 0 1 1 1]\n",
      "[1 1 0 1 1 1 1 0]\n",
      "[1 0 0 0 0 0 1 1]\n",
      "[0 1 0 1 1 1 0 1]\n",
      "[0 0 0 0 0 0 1 1]\n",
      "[1 0 1 0 0 1 0 0]\n",
      "[0 0 0 1 1 0 0 0]\n",
      "[0 1 0 0 1 0 1 1]\n",
      "[0 1 1 1 1 0 1 1]\n",
      "[1 0 1 1 1 1 1 0]\n",
      "[0 0 0 1 1 1 0 1]\n",
      "[0 0 0 0 1 1 0 1]\n",
      "[0 1 0 0 1 0 0 0]\n",
      "[0 1 1 1 1 0 0 0]\n",
      "[0 1 1 1 0 0 1 0]\n",
      "[0 0 1 1 1 0 0 0]\n",
      "[0 1 0 1 0 0 0 0]\n",
      "[0 0 0 1 0 0 0 0]\n",
      "[0 0 0 1 1 0 0 0]\n",
      "[0 0 1 1 1 0 0 0]\n",
      "[1 0 1 0 0 0 0 0]\n",
      "[0 0 1 0 0 1 0 0]\n",
      "[0 0 0 0 1 0 1 1]\n",
      "[1 0 0 0 1 0 1 1]\n",
      "[1 0 1 0 1 0 1 0]\n",
      "[1 0 1 0 0 1 0 1]\n",
      "[0 0 0 1 1 1 0 1]\n",
      "[0 1 0 1 1 0 0 1]\n",
      "[1 1 1 1 0 0 1 1]\n",
      "[1 1 1 1 0 1 1 0]\n",
      "[1 1 1 1 1 1 0 0]\n",
      "[1 1 0 0 1 1 0 0]\n",
      "[0 1 0 1 0 1 0 0]\n",
      "[1 0 0 1 1 0 0 0]\n",
      "[0 0 1 0 0 0 0 0]\n",
      "[1 1 1 0 0 0 0 1]\n",
      "[0 0 0 1 0 0 1 0]\n",
      "[0 1 1 1 0 1 1 1]\n",
      "[1 0 0 0 1 1 0 1]\n",
      "[1 0 1 1 1 1 1 1]\n",
      "[1 0 0 1 0 1 0 1]\n",
      "[0 1 0 0 1 0 1 1]\n",
      "[0 0 1 0 1 1 1 0]\n",
      "[0 0 1 1 1 0 1 0]\n",
      "[1 0 0 0 0 0 0 0]\n",
      "[1 0 1 1 1 0 1 0]\n",
      "[0 0 0 1 0 0 1 0]\n",
      "[1 0 0 0 0 0 1 0]\n",
      "[0 0 1 0 0 0 1 0]\n",
      "[1 1 1 1 1 1 1 1]\n",
      "[0 1 0 0 1 1 1 0]\n",
      "[1 1 1 0 0 1 1 1]\n",
      "[0 1 0 0 1 0 0 0]\n",
      "[1 0 1 1 0 0 0 1]\n",
      "[1 1 1 1 0 0 0 1]\n",
      "[1 0 1 1 0 0 0 0]\n",
      "[0 0 1 1 0 0 0 1]\n",
      "[0 0 1 0 0 0 0 0]\n",
      "[0 1 1 0 0 1 1 1]\n",
      "[1 1 0 1 0 0 0 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|████████████████████████████████████████████████████████████████████████████| 1/1 [01:52<00:00, 112.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0 0 1 0 0 0]\n",
      "[1 0 0 1 1 0 1 0]\n",
      "[1 0 0 1 0 0 1 0]\n",
      "[0 0 1 0 0 0 1 0]\n",
      "0.557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "epochs = 1\n",
    "grad_acc_steps = 1\n",
    "train_loss_values = []\n",
    "dev_acc_values = []\n",
    "\n",
    "for _ in tqdm(range(epochs), desc=\"Epoch\"):\n",
    "\n",
    "    # Training\n",
    "    epoch_train_loss = 0 # Cumulative loss\n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        input_ids = batch[0].to(device)\n",
    "        attention_masks = batch[1].to(device)\n",
    "        labels = batch[2].to(device)     \n",
    "\n",
    "        outputs = model(input_ids, token_type_ids=None, attention_mask=attention_masks, labels=labels)\n",
    "#         outputs = model(input_ids, attention_mask=attention_masks, labels=labels)\n",
    "\n",
    "        loss = outputs[0]\n",
    "        loss = loss / grad_acc_steps\n",
    "        epoch_train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        if (step+1) % grad_acc_steps == 0: # Gradient accumulation is over\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # Clipping gradients\n",
    "            optimizer.step()\n",
    "            model.zero_grad()\n",
    "\n",
    "    epoch_train_loss = epoch_train_loss / len(train_dataloader)          \n",
    "    train_loss_values.append(epoch_train_loss)\n",
    "\n",
    "    # Evaluation\n",
    "    epoch_dev_accuracy = 0 # Cumulative accuracy\n",
    "    model.eval()\n",
    "\n",
    "    for batch in dev_dataloader:\n",
    "\n",
    "        input_ids = batch[0].to(device)\n",
    "        attention_masks = batch[1].to(device)\n",
    "        labels = batch[2]\n",
    "\n",
    "        with torch.no_grad():        \n",
    "            outputs = model(input_ids, token_type_ids=None, attention_mask=attention_masks)\n",
    "#             outputs = model(input_ids, attention_mask=attention_masks)\n",
    "\n",
    "        logits = outputs[0]\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "\n",
    "        predictions = np.argmax(logits, axis=1).flatten()\n",
    "        labels = labels.numpy().flatten()\n",
    "\n",
    "        epoch_dev_accuracy += np.sum(predictions == labels) / len(labels)\n",
    "        print(predictions)\n",
    "        print(labels)\n",
    "\n",
    "    epoch_dev_accuracy = epoch_dev_accuracy / len(dev_dataloader)\n",
    "    dev_acc_values.append(epoch_dev_accuracy)\n",
    "    print(epoch_dev_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afc60ebd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.557"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch_dev_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825239ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case of GPU OOM errors, this may fix the problem, otherwise decrease batch size.\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a577890a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "torch.save(model.state_dict(), \"../models/4kRobertaBool576.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0a4212",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0b1bcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdf5f63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b37cd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
